{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05d60d4-0f4c-4253-a13c-e30b443ca08f",
   "metadata": {},
   "source": [
    "# Prototipazione Rapida con Open Data e Machine Learning\n",
    "## Cremona, 16/12/2022\n",
    "### Dr. Ir. Jacopo De Stefani - `J.deStefani@tudelft.nl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbc140-0a42-4a12-b1b4-7ac7f575b30a",
   "metadata": {},
   "source": [
    "# Outline of the workshop\n",
    "\n",
    "**1st hour: Introduction to Open Data**\n",
    "- 10-20 min theoretical introduction\n",
    "- 30-40 min hands on exercises exploring/gathering open data\n",
    "**2nd hour: Introduction to Data Analytics**\n",
    "- 10-20 min theoretical introduction\n",
    "- 30-40 min hands on exercises on data preprocessing and visualization\n",
    "- 5-10 min break\n",
    "\n",
    "**Longer break: 15 minutes**\n",
    "\n",
    "**3rd hour: Introduction to ML/AI**\n",
    "- 10-20 min theoretical introduction\n",
    "- 30-40 min hands on exercises implementing ML models using Sklearn/Keras\n",
    "- 5-10 min break\n",
    "**4th hour: Wrap-up and publication of the final work**\n",
    "- 10-20 min theoretical introduction\n",
    "- 30-40 min Wrapping up and finalizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e741e-5149-4984-a891-6e388ecd3bec",
   "metadata": {},
   "source": [
    "# 1. Introduction to Open Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea9e27-7139-4a6d-b236-a8411b95f70f",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "\n",
    "### Global websites\n",
    "- Proprietary sources\n",
    "    - Kaggle\n",
    "    - Google Datasets\n",
    "    - …\n",
    "- Governmental sources\n",
    "    - EU: https://data.europa.eu/en\n",
    "    - US: https://www.data.gov/\n",
    "    - …\n",
    "- Academic sources\n",
    "    - UCI Machine Learning Repository\n",
    "    - …\n",
    "\n",
    "### National websites\n",
    "- Governmental sources\n",
    "    - https://github.com/italia/awesome-italian-public-datasets\n",
    "    - https://www.kaggle.com/general/27278\n",
    "\n",
    "### Local websites\n",
    "- Governmental sources\n",
    "    - https://www.dati.lombardia.it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee3159-318f-4794-a204-f6272a80e16f",
   "metadata": {},
   "source": [
    "## Activity 1.1\n",
    "\n",
    "Explore the different data sources to find a dataset which is particularly relevant to you.\n",
    "If this seems overwhelming, you can have a look in the `data` folder for some pre-selected examples.\n",
    "\n",
    "Try to answer the following questions:\n",
    "- In which format is this dataset?\n",
    "- Which computer program can you use to open this data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568245c-bfd2-4d7c-b01e-44d678504b84",
   "metadata": {},
   "source": [
    "Write your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a013e08-5ca4-45bd-b171-c39261b6cbea",
   "metadata": {},
   "source": [
    "## Activity 1.2\n",
    "\n",
    "We are now going to try to import the different data types in Python.\n",
    "1. Have a look at the documentation of the [Pandas](https://pandas.pydata.org/) library (and specifically the `read_` functions) in order to determine how to use it.\n",
    "2. Try to use these function to read data into your Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5436da-eb47-4a60-8c25-06ff99b4f905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1995e018-33e3-4827-bd68-a9695e1a3809",
   "metadata": {},
   "source": [
    "## Activity 1.3\n",
    "\n",
    "Let's focus on a CSV dataset.\n",
    "1. Store it in a Python variable as a Pandas DataFrame.\n",
    "2. Use the Pandas built-in functions (`info, describe, value_counts, nunique`) to provide a summary of the dataset.\n",
    "3. Try to select the first 5 lines of the dataset.\n",
    "4. Try to select the first 3 columns of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bcefb6-d52b-4e4c-9e1a-c21193cab8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8fa4825-86a9-4a7c-8b15-c3bce1722722",
   "metadata": {},
   "source": [
    "# 2. Introduction to Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b40032-99c1-446f-903e-5ae967ea4e1e",
   "metadata": {},
   "source": [
    "## Activity 2.1\n",
    "\n",
    "Let's now focus on a [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset?select=healthcare-dataset-stroke-data.csv)\n",
    "that we had previously imported in the `stroke_df` variable.\n",
    "We are going to use the `pandas` library to perform some exploratory understanding of the data.\n",
    "\n",
    "1. Load the dataset in the `stroke_df` variable\n",
    "2. Display the content of the `stroke_df` variable\n",
    "3. What are the type of the different columns? Use the knowledge from `pandas` to determine the type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51465cc6-6681-45bc-a837-c85cd332049a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35b22292-84a0-4184-a181-a609093363b7",
   "metadata": {},
   "source": [
    "## Activity 2.2\n",
    "\n",
    "Two type of variables occur in this dataset:\n",
    "- Numeric: `int64` and `float64`\n",
    "- Categorical: `object`\n",
    "\n",
    "We are going to use the `seaborn` library to perform some exploratory visualizations.\n",
    "Have a look at the documentation of the [Seaborn](https://seaborn.pydata.org/) library and the Gallery to get some inspiration for some plots.\n",
    "According to the type of variables, different visualizatons can be used\n",
    "\n",
    "- Numeric - Numeric: Scatterplot\n",
    "- (Numeric - Categorical)/(Categorical - Numeric): Boxplot, Scatterplot, Violin plot\n",
    "\n",
    "1. Use the Seaborn library to explore the data. \n",
    "2. Try to answer to some of these questions using the seaborne library:\n",
    "    1) Who has more strokes between Male/Female?\n",
    "    2) People of which age group are more likely to get a stroke?\n",
    "    3) Is hypertension a cause?\n",
    "    4) Is A person with heart disease more likely to get a stroke ?\n",
    "    5) May Marriage be a cause of strokes ?\n",
    "    6) Are people working in private jobs the majority of people with strokes (mostly because of stress) ?\n",
    "    7) Do people living in urban areas have more chances of getting stroke?\n",
    "    8) Are glucose levels a symptom of stroke ? Or of a pathology, such as obesity, leading to stroke ?\n",
    "    9) What is the relationship between BMI, age and gender?\n",
    "    10) Are people who smoke more likely to get a stroke ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49163a-190b-4b99-8eda-bf8d23ca871a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7964a74-2617-4d76-8c28-7a8dc64b5c81",
   "metadata": {},
   "source": [
    "## Activity 2.3\n",
    "\n",
    "A common problem in many datasets is missing or incomplete data, usually indicated by N/A, NA, NaN, or extreme values.\n",
    "\n",
    "1. Is there any column containing missing data in this dataset?\n",
    "2. If there are any, display the column(s) containing missing data.\n",
    "\n",
    "Several ways exist to deal with incomplete or missing data, the most common being:\n",
    "- Removing the lines having missing/incomplete data\n",
    "- Replacing the missing/incomplete data by meaningful values (e.g. the mean, the median, a numeric values signalling the error)\n",
    "\n",
    "3. Count the number of missing values in the column(s) containing missing data.\n",
    "4. In this case, we are going to drop the missing values using the `dropna` function from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a4eb4-c09d-41cb-a267-70f9a453ef10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a30a421c-910a-488d-8d2a-a51a461352fd",
   "metadata": {},
   "source": [
    "# 3. Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cac880-084a-4c2a-b9ab-d78ec08a49a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Activity 3.1\n",
    "\n",
    "In order to apply Machine Learning on the [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset?select=healthcare-dataset-stroke-data.csv) that we had previously imported in the `stroke_df` variable, we need to perform the following operations:\n",
    "\n",
    "1. Impute missing values (Done in 2.3 by dropping the missing values)\n",
    "2. Split data into training and test using the `train_test_split` function (3.1)\n",
    "3. Transform categorical variables (3.2)\n",
    "\n",
    "Please note that the transformation in categorical variables needs to be done after the split into training and test set in order to avoid information leakage (normally the testing set should not be seen by the model during its training phase).\n",
    "We are going to use the `scikit-learn` library to perform most of the split and transformation tasks.\n",
    "\n",
    "Here you need to:\n",
    "1. Divide the `stroke_noNA_df` dataset into two variables:\n",
    "- `X` containing the input variables\n",
    "- `Y` containing the target variable\n",
    "2. Use the `train_test_split` function to obtain `X_train, X_test, Y_train, Y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a8ab5-3b92-4163-8141-785d218022d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb65947c-93b1-4e2f-a6d9-2bde3f426683",
   "metadata": {},
   "source": [
    "## Activity 3.2\n",
    "\n",
    "Before inputting the data to a Machine Learning model, we need all the inputs to be numeric.\n",
    "In order to transform categorical data into numeric ones, three techniques exist (cf. https://www.kaggle.com/code/alexisbcook/categorical-variables):\n",
    "- Dropping Categorical variables\n",
    "- Ordinal Encoding: A categorical variable is replaced by a single numerical variable, where each category is mapped to a different, increasing integer value.\n",
    "- One-hot Encoding: A categorical variable with $n$ different categories is replaced by $n$ binary variables, each of them corresponding to a category. \n",
    "\n",
    "We are going to use the `scikit-learn` library to perform the transformation of the variables and to subsequently fit the models.\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library \n",
    "2. Have a look at the following code to perform the transformation of categorical variables:\n",
    "- Dropping Categorical variables: `drop_X_train` and `drop_X_test`\n",
    "- Ordinal Encoding: `label_X_train` and `label_X_test`\n",
    "- One-hot Encoding: `OH_X_train` and `OH_X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acedc5-d30c-4138-97ec-1a3fdee768b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d1604-97ae-4364-9e85-72d33dd3f4b7",
   "metadata": {},
   "source": [
    "### Dropping categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279807b-bd38-4ff6-8d0a-0d068612eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_test = X_test.select_dtypes(exclude=['object'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b2978-d4df-4f56-b8c7-5c56535118ed",
   "metadata": {},
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef11d5-cc65-4c07-94d5-8cc9cb93b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_test = X_test.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_test[object_cols] = ordinal_encoder.transform(X_test[object_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0523597-02ba-4e86-88f2-aa46f6b59a56",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9754e1-eb95-48cb-8165-cd552bb21eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pandas.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_test = pandas.DataFrame(OH_encoder.transform(X_test[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_test.index = X_test.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_test = X_test.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pandas.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_test = pandas.concat([num_X_test, OH_cols_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62428256-00aa-4c0b-8117-8bda733cdafb",
   "metadata": {},
   "source": [
    "## Activity 3.3\n",
    "\n",
    "Finally, we are able to fit some models using the `scikit-learn` library.\n",
    "As a starter, we will will be using a [Naive Bayesian Model](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library for the Naive Bayes model.\n",
    "2. Initialize the model\n",
    "3. Use the `fit` function to perform the training of the model on the training set\n",
    "4. Use the `predict` function to perform the prediction of the model on the test set\n",
    "5. Use the `accuracy_score, balanced_accuracy_score, f1_score` to compare the predictions with the actual values and obtain some performance metrics about the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193065c-f27c-4349-b647-4766445edb94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b08c5f25-d818-41fe-9c2e-b9255cbc8021",
   "metadata": {},
   "source": [
    "## Activity 3.4\n",
    "\n",
    "Now that you are familiar with the pipeline of training, testing and evaluating one model, you can easily repeat the procedure for multiple models.\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library for other classification models\n",
    "2. Initialize the model\n",
    "3. Use the `fit` function to perform the training of the model on the training set\n",
    "4. Use the `predict` function to perform the prediction of the model on the test set\n",
    "5. Use the `accuracy_score, balanced_accuracy_score, f1_score` to compare the predictions with the actual values and obtain performance metrics about the models. \n",
    "    a. Are there any differences in the values of the metrics?\n",
    "    b. Why are these values different? Check the documentation to get to know more about the metrics.\n",
    "6. Create a dictionary/Data Frame in order to be able to compare the performance scores of the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72275f84-0626-432d-83a0-4a849fa61c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da3264e8-d49b-44f2-b3b8-2c61ae96bd8e",
   "metadata": {},
   "source": [
    "## Activity 3.5\n",
    "\n",
    "Congratulations! By now you should be able to train, test and evaluate multiple models.\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library for the parameters of other classification models.\n",
    "2. Try to adjust the performances of the different models to see the impact that thoese parameters have on the performances of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb099e89-e6de-4b68-a1ef-286c0f298772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13a6329a-fd5b-47fc-851a-f7729e0f8c30",
   "metadata": {},
   "source": [
    "# 4. Wrap-up and publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09c0df-f912-402a-be46-0d404c0708e8",
   "metadata": {},
   "source": [
    "By following the different steps presented in this workshop, you will now have:\n",
    "\n",
    "- An Open Data source\n",
    "- A Dataset (and intermediate transformations)\n",
    "- Visualization of the most relevant characteristics\n",
    "- One or model trained Model\n",
    "\n",
    "You are now ready to publish your content!\n",
    "\n",
    "In order to ensure the reproducibility of your code, you should upload both the code and the data on a public platform online.\n",
    "In order to do so, there are the following options:\n",
    "- Public repository on [Github](https://github.com/)\n",
    "- Public dataset and related notebook on [Kaggle](https://www.kaggle.com/datasets)\n",
    "- [Google Colab](https://colab.research.google.com/)\n",
    "\n",
    "We will be seeing how to perform the upload on Github by:\n",
    "\n",
    "1. Structuring the content locally to separate code and data\n",
    "2. Create an empty public repository on Github with a README\n",
    "3. Cloning the empty repository on your PC\n",
    "4. Adding your content\n",
    "5. Committing your content\n",
    "6. Pushing your content on Github\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c32ff-a586-4726-aae4-a9cc267b02e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
